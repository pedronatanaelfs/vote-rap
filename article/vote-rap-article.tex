% Article: Vote Outcome Prediction using Temporal Evidence of Related Approval Patterns
% Based on ESWA template
% Authors: Pedro N. F. da Silva and colleagues

\documentclass[review]{elsarticle}
\graphicspath{ {./figures/} }
\usepackage{hyperref}
\usepackage{float}
\usepackage{verbatim} %comments
\usepackage{apalike}
\restylefloat{figure}
\floatstyle{plaintop} %table caption at top
\restylefloat{table}
\usepackage{array}
\usepackage{amsmath}


\journal{Expert Systems with Applications}

%% For ESWA journal you need to use APA style
\bibliographystyle{model5-names}\biboptions{authoryear}

\begin{document}
\begin{frontmatter}

\title{Legislative Vote Outcome Prediction Using Temporal Approval Patterns}

\author[label1,cor1]{Pedro N. F. da Silva}
\ead{pnfs@ecomp.poli.br}

\author[label2]{Dimas Cassimiro Nascimento}
\ead{dimas.cassimiro@ufape.edu.br}

\author[label3]{Bruno Nogueira}
\ead{bruno@ic.ufal.br}

\cortext[cor1]{Corresponding author.}

\address[label1]{Programa de Pós-Graduação da Universidade de Pernambuco, Garanhuns, Pernambuco, Brazil}
\address[label2]{Universidade Federal do Agreste de Pernambuco, Garanhuns, Pernambuco, Brazil}
\address[label3]{Universidade Federal de Alagoas, Maceió, Alagoas, Brazil}

\begin{frontmatter}

\begin{abstract}
Legislative vote-outcome prediction in multiparty systems is challenging due to complex coalition dynamics and temporal dependencies. We propose VOTE-RAP, a feature-based approach for predicting whether propositions are approved or rejected in the Brazilian Chamber of Deputies, using leakage-safe temporal and structural signals derived from legislative history. We construct a dataset of 9,260 voting sessions from 2003 to 2024 and train an XGBoost classifier evaluated with a chronological 80/20 train--test split. In a reproducible comparison against simple baselines and a VIOLA-style data-centric methodology adapted from \citet{Viola2022DataCentricBrazil}, VOTE-RAP achieves AUROC 0.908 and rejected-class F1 0.703, outperforming both the government-orientation baseline (AUROC 0.739, F1 0.637) and the VIOLA-style model (AUROC 0.834, F1 0.655).
\end{abstract}


\begin{keyword}
Legislative vote prediction \sep Machine learning \sep Temporal features \sep Brazilian Chamber of Deputies \sep Gradient-boosted trees \sep Political behavior analysis
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{introduction}

The Brazilian Chamber of Deputies, with its 513 members, operates in a complex political environment shaped by multiparty competition, shifting coalitions, and strong temporal variation in alliances. Predicting whether a proposition will be approved or rejected in this setting requires methods that can capture institutional signals and historical patterns while remaining robust to changes over time. The growing availability of open legislative data, including roll-call votes, proposition metadata, and deputy and party information, has enabled multiple computational approaches for modeling legislative outcomes.

Existing approaches commonly fall into two groups. The first relies on embeddings and neural architectures to model roll-call behavior or to incorporate textual information \citep{Patil2020,Kraft2021}. These methods can capture rich representations, but they often demand substantial modeling complexity and may be harder to audit, which limits their usefulness when analysts need a clear explanation for a predicted approval or rejection. The second group leverages complex network analysis to characterize relationships among legislators and voting blocs \citep{Brito2020,Ferreira2021,Cherepnalkoski2022}. Network methods provide valuable structural insight, but they typically require additional modeling steps to translate network structure into outcome-level predictions and their performance can be sensitive to graph construction choices and temporal instability in coalitions.

In this work, we argue that interpretable, temporally grounded features derived from legislative history can provide strong predictive signal while preserving transparency and operational simplicity. In particular, three types of information are central. Government orientation captures an institutional stance that can be highly informative when available. Party popularity summarizes short-term approval momentum of the party, or of an institutional author type when the author is not a deputy, reflecting shifts in coalition support. Historical Approval Rate (HAR) captures proposition-specific memory by aggregating outcomes from prior voting sessions of the same proposition, providing an interpretable prior when repeated votes occur. Together with coalition-size indicators, these signals encode institutional, historical, and temporal dynamics that influence outcomes beyond static ideological positioning.

Beyond simple baselines, we also seek comparison against a stronger published reference that is feasible under our data constraints. Among the related studies, \citet{Viola2022DataCentricBrazil} is the closest match in modeling philosophy and empirical goal: it proposes a \emph{data-centric}, feature-driven supervised pipeline for prediction in the Brazilian multi-party legislature, emphasizing engineered heterogeneous signals and interpretability (including feature-importance and SHAP-based analysis) rather than end-to-end neural representation learning. In their setup, the authors evaluate LightGBM models and report test AUROC values in the high 0.87--0.88 range for different feature configurations, including SHAP-derived meta-features. While their prediction target is formulated at the individual level (predicting adherence/stance in roll-call voting scenarios), it provides a strong and auditable feature-based benchmark that we can adapt and evaluate using only pre-vote information. 

Motivated by these considerations, we propose VOTE-RAP (Vote Outcome Prediction using Temporal Evidence of Related Approval Patterns), a feature-based machine learning approach that uses the above signals in a supervised classifier. Our main contributions are: (i) a compact set of leakage-safe temporal and structural features for proposition-level outcome prediction; (ii) a reproducible evaluation protocol based on chronological splitting and year-by-year analysis; and (iii) an external comparison via a VIOLA-style data-centric methodology adapted to the same task and metrics.



\section{Related Work}
\label{related_work}

Research on legislative behavior modeling and prediction spans political science, natural language processing, and network science. In political science, roll-call votes are often studied through latent ideology and ideal-point models \cite{Poole1985,Clinton2004}. These frameworks are highly interpretable and support substantive analysis, but they are not always designed as end-to-end predictors of proposition-level outcomes.

In NLP and machine learning, a major trajectory focuses on predicting votes using text and representation learning. Early work demonstrated that bill text can be predictive of roll-call behavior \cite{Gerrish2011}, and later studies advanced this line with embedding-based and neural architectures \citep{Kraft2021,Patil2020}. Recent representation learning approaches further incorporate political-actor and social-context signals to build richer embeddings that improve downstream tasks, including vote prediction \citep{Feng2022PAR}. These approaches can leverage semantic and contextual signals, but they often increase modeling complexity and reduce auditability, which can be a limitation when practitioners require transparent explanations for approval or rejection. Related work also incorporates heterogeneous signals such as legislator and party attributes and external knowledge to improve generalization and address cold-start settings \citep{Kornilova2018,Cheng2017,MultiFactor2019}.

A parallel line of work frames the problem as supervised prediction using structured, engineered signals. In a multi-party setting, data-centric pipelines combine heterogeneous features and emphasize empirical rigor, including feature analysis and explainability tooling, to predict legislative outcomes \citep{Viola2022DataCentricBrazil}. Other studies target bill-level success rather than individual roll-call behavior, predicting whether legislation advances through the process using machine learning and engineered institutional features \citep{Bari2021AIBillPassageUS}. These formulations broaden the scope of prediction targets but can differ substantially from proposition-level outcome prediction at the time of voting, and they may rely on signals that are not consistently available across legislative systems.

A complementary trajectory uses complex networks to study coalitions, polarization, and voting structure. These works build vote similarity or signed networks to identify communities and alliance dynamics \citep{Brito2020,Ferreira2021,Cherepnalkoski2022,Arinik2020}. Network-based perspectives provide valuable descriptive insight, but they often require additional modeling steps to translate structural patterns into proposition-level outcome predictions, and results can depend on graph construction choices and temporal instability in coalitions.

Table~\ref{tab:related_work_comparison} summarizes representative directions and highlights how VOTE-RAP differs in task definition, model choice, and feature design, as well as which prior approaches are most feasible to serve as comparative baselines under our available inputs.

\begin{table}[H]
\caption{Related work vs. VOTE-RAP by task, model family, and signals.}

\centering
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|>{\centering\arraybackslash}m{3.6cm}|
                >{\centering\arraybackslash}m{2.6cm}|
                >{\centering\arraybackslash}m{2.8cm}|
                >{\centering\arraybackslash}m{3.6cm}|}
\hline
\textbf{Work} & \textbf{Task} & \textbf{Model family} & \textbf{Primary signals} \\ \hline
\citep{Poole1985,Clinton2004} & Ideology estimation and roll-call structure & Statistical latent models & Voting patterns, latent ideology dimensions \\ \hline
\citep{Gerrish2011} & Vote prediction using text & Text-based predictive model & Bill text features \\ \hline
\citep{Kraft2021,Patil2020,Feng2022PAR} & Vote prediction with learned representations & Neural representation learning & Text embeddings; learned legislator/bill/actor representations; social/expert context \\ \hline
\citep{Kornilova2018,Cheng2017,MultiFactor2019} & Vote prediction with heterogeneous signals & Hybrid ML and neural & Text plus legislator attributes, party signals, external or structured knowledge \\ \hline
\citep{Viola2022DataCentricBrazil} & Predicting outcomes in multi-party legislatures & Feature-based supervised ML & Engineered heterogeneous features; data-centric evaluation and explainability \\ \hline
\citep{Bari2021AIBillPassageUS} & Bill-level outcome prediction (process success) & Supervised ML / ensembles & Engineered institutional and bill metadata features \\ \hline
\citep{Brito2020,Ferreira2021,Cherepnalkoski2022,Arinik2020} & Coalition and polarization analysis & Network analysis & Vote similarity networks, community structure, bloc dynamics \\ \hline
\textbf{VOTE-RAP} & Proposition-level outcome prediction & Feature-based supervised ML (XGBoost) & Government orientation, party popularity (recency), proposition history (HAR), coalition-size indicators \\ \hline
\end{tabular}
\label{tab:related_work_comparison}
\normalsize
\end{table}

Among the directions in Table~\ref{tab:related_work_comparison}, we choose \citet{Viola2022DataCentricBrazil} as the primary external comparison because it is the closest match to our problem setting and modeling philosophy: it targets prediction in the Brazilian multi-party legislature and adopts a feature-based supervised learning pipeline with a data-centric emphasis and explicit interpretability analysis. In contrast, representation-learning approaches (e.g., \citealp{Kraft2021,Patil2020,Feng2022PAR}) rely primarily on textual inputs and additional context signals that are not consistently available in our vote-only setting, while bill-level process-success prediction (e.g., \citealp{Bari2021AIBillPassageUS}) depends on legislative-stage metadata beyond roll-call outcomes. Network-analysis studies provide important descriptive insight into coalition structure, but converting them into proposition-level outcome predictors introduces additional modeling assumptions (e.g., graph construction and temporal updating choices) that can confound direct benchmarking. For these reasons, we benchmark against a reproduction of the core Viola-style feature-based approach, adapted to our proposition-level outcome prediction task and evaluated under the same leakage-safe temporal protocol used for VOTE-RAP.

Our approach differs by targeting proposition-level outcomes using structured temporal evidence of related approval patterns rather than focusing primarily on individual deputy vote prediction or relying on static network structure alone. By constraining all features to information available prior to each vote, we preserve temporal integrity while remaining auditable, since each signal can be traced to explicit institutional or historical quantities.


\section{Data Collection and Preparation}
\label{data}

We construct our dataset from official open data released by the Brazilian Chamber of Deputies. We integrate roll-call vote records at the deputy level with proposition metadata and legislature and session identifiers, producing a proposition-level outcome dataset while preserving the temporal ordering required for realistic prediction.

The sources include (i) roll-call voting records, which provide deputy votes for each session, (ii) proposition metadata, including type, authorship, and key dates, and (iii) deputy and party attributes and legislature identifiers to support party-based signals and time indexing. We harmonize identifiers across sources and resolve entity mappings (proposition, deputy, party, legislature) to ensure consistent linkage between votes, propositions, and their political context.

To form a clean prediction dataset, we retain only sessions associated with propositions that have an unambiguous final outcome (approved or rejected), excluding cases with missing or unclear status. To prevent temporal leakage, all engineered features for a voting session are computed strictly from information available prior to that session, and any rolling statistics (e.g., party popularity, HAR) are computed using only earlier sessions. We then adopt a chronological evaluation protocol, splitting the data into 80\% training (7,131 sessions) and 20\% testing (1,783 sessions) without shuffling. Figure~\ref{fig:data_pipeline} summarizes the dataset construction and leakage-safe feature computation workflow.

The final dataset contains 9,260 voting sessions spanning 2003 to 2024 (legislatures 52 to 57) and exhibits class imbalance, with approximately 79.5\% approved propositions and 20.5\% rejected propositions. Additional descriptive trends are reported through aggregated results in Section~\ref{results}.

\begin{figure}[H]
\centering
\includegraphics[width=0.90\textwidth]{data_pipeline.png}
\caption{Dataset construction and leakage-safe feature computation workflow, from raw Chamber open-data sources to the final chronological train-test split.}
\label{fig:data_pipeline}
\end{figure}


\section{Feature Engineering}
\label{features}

Let each voting session be indexed by $i \in \{1,\dots,N\}$, with timestamp $t_i$, proposition identifier $p_i \in \mathcal{P}$, and binary outcome $y_i \in \{0,1\}$, where $y_i=1$ denotes approval and $y_i=0$ denotes rejection. To enforce temporal integrity, all features for session $i$ are computed using only sessions $j$ such that $t_j < t_i$. We define a mapping $g(i)$ that assigns each session to a group label representing either the author’s party (when the author is a deputy) or the institutional author type (otherwise). We also use authorship metadata available prior to voting to derive coalition-size indicators. The proposed features are then defined as follows.

\subsection{Government Orientation}
Government orientation encodes the executive recommendation associated with the session. The raw data provides two fields, denoted $G^{(1)}_i$ (GOV.) and $G^{(2)}_i$ (Governo), each taking values in $\{-1,0,1\}$. We resolve them into a single indicator $x^{\mathrm{gov}}_i \in \{-1,0,1\}$ using the deterministic rule
\[
x^{\mathrm{gov}}_i =
\begin{cases}
G^{(1)}_i, & \text{if } G^{(1)}_i = G^{(2)}_i,\\
G^{(1)}_i, & \text{if } G^{(1)}_i \neq 0,\\
G^{(2)}_i, & \text{otherwise.}
\end{cases}
\]
Values $-1,0,1$ represent recommendation against, neutral or undefined, and in favor of approval, respectively. When this information is unavailable, the feature is treated as missing and handled as described in Section~\ref{modeling}.


\subsection{Party Popularity}
To capture short-term momentum, we define party popularity as a leakage-safe recency-based approval rate for the group label $g(i)$. Let $\mathcal{H}_i(g)$ be the set of indices of prior sessions for group $g$:
\[
\mathcal{H}_i(g) = \{ j : t_j < t_i \ \wedge \ g(j)=g \}.
\]
We use a fixed recency window of size $K$ by taking the $K$ most recent elements of $\mathcal{H}_i(g(i))$, denoted $\mathcal{H}^{(K)}_i$. Party popularity is then computed as
\[
x^{\mathrm{pp}}_i =
\begin{cases}
\frac{1}{|\mathcal{H}^{(K)}_i|}\sum\limits_{j \in \mathcal{H}^{(K)}_i} y_j, & \text{if } |\mathcal{H}^{(K)}_i| > 0,\\
0, & \text{otherwise,}
\end{cases}
\]
where the default value 0 is used when no prior session exists for the corresponding group. We choose $K$ via model-selection on temporally ordered training data by comparing AUROC obtained with alternative windows (including full-history and recency-based variants) and selecting the best-performing configuration. In our experiments, $K=5$ yields the best AUROC, and Figure~\ref{fig:party_popularity_window} reports this comparison.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{party_popularity_auroc_comparison.png}
\caption{Party popularity window-size comparison using AUROC on temporally ordered training data. The best-performing configuration is selected for the final definition of \textit{party\_popularity}.}
\label{fig:party_popularity_window}
\end{figure}

\subsection{Historical Approval Rate (HAR)}
Some propositions appear in multiple voting sessions. We capture proposition-specific memory via a leakage-safe historical approval rate conditioned on the proposition identifier. For a given session $i$, define the set of prior sessions that concern the same proposition:
\[
\mathcal{P}_i = \{ j : t_j < t_i \ \wedge \ p_j = p_i \}.
\]
The Historical Approval Rate feature is defined as
\[
x^{\mathrm{har}}_i =
\begin{cases}
\frac{1}{|\mathcal{P}_i|}\sum\limits_{j \in \mathcal{P}_i} y_j, & \text{if } |\mathcal{P}_i| > 0,\\
0.5, & \text{otherwise,}
\end{cases}
\]
where 0.5 is a neutral prior used when the proposition has no recorded previous session. Figure~\ref{fig:har_rules_comparison} compares simple proposition-history heuristics and highlights the Historical Probability Rule, which corresponds exactly to $x^{\mathrm{har}}_i$ with the 0.5 default.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{proposition_history_rules_comparison.png}
\caption{AUROC performance of proposition-history heuristics. The Historical Probability Rule uses the leakage-safe historical mean for the same proposition and defaults to 0.5 when no history exists.}
\label{fig:har_rules_comparison}
\end{figure}

\subsection{Coalition-Size Indicators (Authorship-Based)}
We capture coalition-size effects using authorship metadata available prior to voting. Let $n^{\mathrm{auth}}_i$ denote the number of distinct authors associated with proposition $p_i$ in session $i$. We define two indicators:
\[
x^{\mathrm{na}}_i = \min\{n^{\mathrm{auth}}_i, 10\},
\qquad
x^{\mathrm{gt10}}_i =
\begin{cases}
1, & \text{if } n^{\mathrm{auth}}_i > 10,\\
0, & \text{otherwise.}
\end{cases}
\]
The truncated count $x^{\mathrm{na}}_i$ corresponds to \texttt{num\_authors\_trunc}, limiting sensitivity to extreme author lists while preserving monotonic information. The indicator $x^{\mathrm{gt10}}_i$ corresponds to \texttt{has\_more\_than\_10\_authors} and captures very broad sponsorship. Both features are leakage-safe because they depend only on proposition authorship metadata available before the session outcome is observed.

\subsection{Author Popularity}
In addition to party-level momentum, we incorporate an author-level signal capturing how successful a proposition’s author has been in prior voting outcomes. In our implementation, this feature (\texttt{popularity}) is precomputed from historical voting data and merged into the vote-session dataset by session identifier. Missing values are imputed with 0, representing the absence of prior evidence for the author in the available history.


\section{Modeling Approach}
\label{modeling}

\subsection{Algorithm Selection}
We model proposition-level outcomes with gradient-boosted decision trees using XGBoost, which is well suited to heterogeneous tabular features and can capture nonlinearities and feature interactions without requiring manual feature crossing. The model outputs an approval probability $\hat{p}_i \in [0,1]$ for each session $i$ using the logistic objective.

\subsection{Temporal Train-Test Split}
To preserve temporal realism, we order sessions chronologically by date and split the dataset into an 80\% training prefix and a 20\% test suffix. All model selection and preprocessing decisions are performed using only the training split, and the final model is evaluated once on the held-out test split.

\subsection{Hyperparameter Optimization}
We tune hyperparameters via randomized search over 75 sampled configurations, using AUROC as the selection metric. In our implementation, the search uses 3-fold stratified cross-validation over the training split (with fixed random seed), and the best configuration is retrained on the full training split and evaluated once on the held-out test split.

\subsection{Class Imbalance Handling}
Because rejected propositions are the minority class, we address imbalance by setting \texttt{scale\_pos\_weight} using the training labels in each training fold and in the final training fit. This weighting is computed without using any information from the test split.

\subsection{Preprocessing}
We remove sessions with missing outcome labels (\texttt{aprovacao}). Missing \texttt{party\_\allowbreak popularity} values are imputed with 0, and missing \texttt{historical\_\allowbreak approval\_\allowbreak rate} values are imputed with 0.5, corresponding to a neutral prior when no proposition history exists. Since XGBoost is tree-based, feature scaling is not required, and numeric features are used in their original scale.
\par
For the reproducible comparison runs reported in Section~\ref{results}, we apply standardization to the numeric temporal signals \texttt{popularity}, \texttt{party\_\allowbreak popularity}, and \texttt{historical\_\allowbreak approval\_\allowbreak rate} to keep a consistent preprocessing interface across models.

\subsection{Feature Set}
The final model uses six features: \texttt{popularity} (author popularity), \texttt{gov\_\allowbreak orientation}, \texttt{num\_\allowbreak authors\_\allowbreak trunc}, \texttt{has\_\allowbreak more\_\allowbreak than\_\allowbreak 10\_\allowbreak authors}, \texttt{party\_\allowbreak popularity} computed over the last five prior sessions for the corresponding group, and \texttt{historical\_\allowbreak approval\_\allowbreak rate}.

\section{Results}
\label{results}

\subsection{Simple Baselines}
\label{sec:simple_baselines}
To contextualize model performance, we include four simple baselines for proposition-level outcome prediction: (i) \emph{Random Guess}, which predicts approval or rejection uniformly at random; (ii) \emph{Majority Class}, which always predicts the majority label observed in the training split; (iii) \emph{Stratified Probability}, which predicts approval with probability equal to the training approval rate and rejection otherwise; and (iv) \emph{Government Orientation}, a domain-informed heuristic that predicts approval when \texttt{gov\_\allowbreak orientation}=1, rejection when \texttt{gov\_\allowbreak orientation}=-1, and defaults to the training majority class when \texttt{gov\_\allowbreak orientation}=0 or missing. For AUROC comparisons, we map \texttt{gov\_\allowbreak orientation} to a coarse probability score (0.8 for approval recommendation, 0.2 for rejection recommendation, and the training approval rate otherwise).

\subsection{Baseline Method}
To quantify the contribution of our temporal and structural features, we compare VOTE-RAP against the government-orientation baseline defined in Section~\ref{sec:simple_baselines}. This baseline provides a strong single-signal reference for minority-class performance (rejections) and for discrimination when converted into a coarse probability score.

\subsection{Comprehensive Comparison: Baselines, VOTE-RAP, and a VIOLA-style Method}
\label{sec:comparison_viola}
To enable a direct, metric-aligned comparison, we evaluate (i) the simple baselines in Section~\ref{sec:simple_baselines}, (ii) VOTE-RAP, and (iii) a VIOLA-style data-centric model inspired by \citet{Viola2022DataCentricBrazil}, all under the same chronological 80/20 split and the same evaluation metrics used throughout the paper (AUROC and rejected-class F1).
\par
The VIOLA-style adaptation is constrained to \emph{pre-vote} information to avoid leakage and to be comparable with VOTE-RAP. Concretely, it uses: (a) structured metadata available at or before voting time (e.g., proposition type, author type, legislature, organization, theme, and simple numeric descriptors), (b) proposition text fields (\texttt{ementa}, \texttt{keywords}) encoded with TF--IDF, and (c) compact meta-features derived from global feature-importance information of a base tree model, followed by a light meta-classifier.
\par
Table~\ref{tab:comparison_all_models} reports the results from our reproducible comparison script (\texttt{scripts/02-modeling/compare\_vote\_rap\_vs\_viola.py}), and Figure~\ref{fig:comparison_auroc} and Figure~\ref{fig:comparison_f1_rejected} visualize AUROC and rejected-class F1 ranked from best to worst.

\begin{table}[H]
\caption{Comprehensive test-set comparison (chronological 80/20 split). Metrics reported: Accuracy, F1 for Rejected and Approved classes, and AUROC.}
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{F1 (Rejected)} & \textbf{F1 (Approved)} & \textbf{AUROC} \\ \hline
VOTE-RAP (XGBoost) & 0.930 & 0.703 & 0.960 & 0.908 \\ \hline
VIOLA-style (Structured+Text+Meta) & 0.915 & 0.655 & 0.952 & 0.834 \\ \hline
Government Orientation & 0.922 & 0.637 & 0.956 & 0.739 \\ \hline
Stratified Probability & 0.717 & 0.200 & 0.828 & 0.483 \\ \hline
Random Guess & 0.491 & 0.191 & 0.629 & 0.520 \\ \hline
Majority Class & 0.863 & 0.000 & 0.927 & 0.500 \\ \hline
\end{tabular}
\label{tab:comparison_all_models}
\normalsize
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{comparison_auroc.png}
\caption{AUROC comparison across baselines, VOTE-RAP, and the VIOLA-style method (best at top).}
\label{fig:comparison_auroc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{comparison_f1_rejected.png}
\caption{Rejected-class F1 comparison across baselines, VOTE-RAP, and the VIOLA-style method (best at top).}
\label{fig:comparison_f1_rejected}
\end{figure}


\subsection{Overall Performance}
VOTE-RAP outperforms all baselines and the VIOLA-style method under the same chronological split and metrics. Compared to the government-orientation baseline, VOTE-RAP improves rejected-class F1 (0.703 vs. 0.637) and AUROC (0.908 vs. 0.739 using the coarse probability mapping described in Section~\ref{sec:simple_baselines}). Figure~\ref{fig:comparison_auroc} and Figure~\ref{fig:comparison_f1_rejected} summarize these results.

\subsection{Minority Class Performance}
Rejected propositions form the minority class, making their detection sensitive to class imbalance and to the decision threshold. To prioritize rejected outcomes, we report rejected-class F1 at a threshold selected to maximize rejected-class F1 for the evaluated split. This post-hoc thresholding is used in our repository scripts to emphasize the minority class; a fully leakage-safe alternative would choose the threshold using only training-time validation predictions and then apply it once to the test set.

Under this protocol, VOTE-RAP achieves rejected-class F1 of 0.703, improving upon the government-orientation baseline (0.637). At the selected threshold, precision and recall for the rejected class are 0.837 and 0.611, respectively, indicating that VOTE-RAP identifies rejections more effectively while maintaining high precision. Figure~\ref{fig:pr_curve} provides the precision--recall diagnostic for the minority class.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{precision_recall_curve.png}
\caption{Precision--recall curve for VOTE-RAP on the test set (average precision = 0.9752).}
\label{fig:pr_curve}
\end{figure}

\subsection{Temporal Analysis}
To assess temporal robustness, we perform a year-by-year evaluation using a rolling window. For each test year \(y \in \{2007,\dots,2024\}\), we train an XGBoost model on the three preceding years (\(y-3\) to \(y-1\)) and evaluate it on year \(y\). Class imbalance is handled within each training window via a year-specific \texttt{scale\_pos\_weight} computed from the window’s training labels, and we use the same rejected-class thresholding procedure described in Section~\ref{results}.
\par
Unless stated otherwise, the year-by-year results correspond to the \textbf{enhanced yearly setup}, i.e., the moving-window model that uses the full VOTE-RAP feature set (including \texttt{party\_\allowbreak popularity} and \texttt{historical\_\allowbreak approval\_\allowbreak rate} in addition to \texttt{gov\_\allowbreak orientation}, author popularity, and coalition-size indicators). Across the 18 test years, this enhanced setup attains mean accuracy of 0.811, mean rejected-class F1 of 0.624, and mean AUROC of 0.823, with noticeable variation over time. We observe lower performance in some years (e.g., 2015) and higher performance in others (e.g., 2019--2022), suggesting that predictive effectiveness varies with the legislative and political environment. Figure~\ref{fig:yearly_performance} summarizes year-by-year AUROC and rejected-class F1 with presidential periods highlighted.
\par
We also report a summary comparison against an \textbf{original yearly setup} that follows the same moving-window protocol but omits the two additional temporal features (\texttt{party\_\allowbreak popularity} and \texttt{historical\_\allowbreak approval\_\allowbreak rate}). In our repository, the “original” values shown in Figure~\ref{fig:enhanced_vs_original} are included as a fixed reference from an earlier run/version, while the “enhanced” values are computed from the current enhanced yearly evaluation.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{yearly_performance_with_presidents.png}
\caption{Year-by-year AUROC and F1-score for the rejected class, with Brazilian presidential periods highlighted.}
\label{fig:yearly_performance}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{enhanced_vs_original_comparison.png}
\caption{Enhanced vs. original yearly models: mean performance comparison (accuracy, F1 for approved and rejected classes, and AUROC). The enhanced setup includes \texttt{party\_\allowbreak popularity} and \texttt{historical\_\allowbreak approval\_\allowbreak rate}, while the original setup excludes these features (reference values from an earlier run/version).}
\label{fig:enhanced_vs_original}
\end{figure}

\section{Discussion}
\label{discussion}

\subsection{Interpretability and Transparency}
A key advantage of VOTE-RAP is its auditability: predictions are driven by a small set of explicitly defined features that encode institutional signals (\texttt{gov\_\allowbreak orientation}), coalition-size proxies (\texttt{num\_\allowbreak authors\_\allowbreak trunc} and \texttt{has\_\allowbreak more\_\allowbreak than\_\allowbreak 10\_\allowbreak authors}), and temporal evidence (\texttt{popularity}, \texttt{party\_\allowbreak popularity}, and \texttt{historical\_\allowbreak approval\_\allowbreak rate}). This structure enables direct inspection of why the model favors approval or rejection in a given case and supports transparent debugging when performance varies across time. Consistent with this design, the feature-importance analysis indicates that government orientation provides the strongest single signal, while coalition-size and temporal features contribute additional explanatory power beyond a single-feature baseline.

The temporal features are also interpretable in substantive terms. Party popularity summarizes recent approval success for the proposition’s party (or institutional author type), capturing short-run shifts in legislative support. HAR reflects proposition-specific memory by aggregating outcomes from prior voting sessions of the same proposition; when a proposition has historically been approved (or rejected) in earlier sessions, the model can leverage that information as an informative prior, while defaulting to a neutral value when no history exists.
\par
Finally, our VIOLA-style benchmark incorporates proposition text (TF--IDF) and meta-features but underperforms VOTE-RAP in our setting, suggesting that compact, leakage-safe temporal signals can be more predictive of proposition-level outcomes than text alone under the available data constraints.

\subsection{Limitations}
The temporal evaluation shows that predictive performance is not uniform across years, suggesting sensitivity to changes in the legislative environment (e.g., coalition realignments or major political events). In addition, some predictors have incomplete coverage; in particular, government-orientation recommendations are missing or neutral in many sessions, limiting how often this strong institutional signal can be directly exploited. The \texttt{historical\_\allowbreak approval\_\allowbreak rate} feature is also only informative when propositions recur across multiple sessions; otherwise it defaults to a neutral prior. Finally, our rejected-class thresholding is selected to maximize rejected-class F1 for the evaluated split, which emphasizes minority-class detection but is not a fully leakage-safe deployment protocol unless the threshold is fixed using training-time validation predictions.

\subsection{Future Work}
Future work can extend VOTE-RAP in three directions. First, richer temporal modeling (e.g., adaptive recency windows, temporal ensembling, or time-aware validation schemes) may capture dependencies beyond short fixed windows while maintaining leakage-safe evaluation. Second, incorporating additional context---including proposition text representations (as explored in the VIOLA-style benchmark), committee routing, and other institutional metadata---may improve performance particularly when government orientation is unavailable. Third, multi-task setups that jointly model proposition outcomes and individual votes, as well as network-derived signals from voting graphs, may provide complementary structure and improve robustness during periods of political change.

\section{Conclusion}
\label{conclusion}

We presented VOTE-RAP, a feature-based approach for predicting proposition-level vote outcomes in the Brazilian Chamber of Deputies using leakage-safe temporal and structural signals, including \texttt{gov\_\allowbreak orientation}, coalition-size proxies, author popularity, party popularity (recency), and proposition-level historical approval rate (HAR). In our comprehensive benchmark under a chronological 80/20 split, VOTE-RAP achieves AUROC 0.908 and rejected-class F1 0.703, outperforming both the government-orientation baseline (AUROC 0.739, F1 0.637) and a VIOLA-style data-centric model adapted from \citet{Viola2022DataCentricBrazil} (AUROC 0.834, F1 0.655). Finally, a year-by-year moving-window analysis shows that performance varies across political periods, underscoring the importance of temporal evaluation in dynamic multiparty settings.

\section*{CRediT authorship contribution statement}
\textbf{Pedro N. F. da Silva:} Conceptualization, Methodology, Software, Data curation, Formal analysis, Investigation, Visualization, Writing (original draft), Writing (review and editing). 
\textbf{Dimas Cassimiro Nascimento:} Supervision, Methodology, Validation, Writing (review and editing). 
\textbf{Bruno Nogueira:} Supervision, Methodology, Validation, Writing (review and editing).


\begin{comment}
For transparency, we require corresponding authors to provide co-author contributions to the manuscript using the relevant CRediT roles. The CRediT taxonomy includes 14 different roles describing each contributor's specific contribution to the scholarly output. The roles are: Conceptualization; Data curation; Formal analysis; Funding acquisition; Investigation; Methodology; Project administration; Resources; Software; Supervision; Validation; Visualization; Roles/Writing - original draft; and Writing - review & editing. Note that not all roles may apply to every manuscript, and authors may have contributed through multiple roles.
\end{comment}

\section*{Declaration of Competing Interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section*{Acknowledgements}
We acknowledge the Brazilian Chamber of Deputies for providing open access to legislative data. We also acknowledge the University of Pernambuco for institutional support. This work was supported by FACEPE, which enabled a research visit to Tampere University. We thank Professor Kostas Stefanidis for the reception and support during the visit, and we thank all colleagues and collaborators who provided valuable feedback on this work.


\section*{Data availability}
The dataset and code used in this study are available at \url{https://github.com/pedronatanaelfs/vote-rap}. The repository includes voting sessions, proposition metadata, and the engineered features used for model training and evaluation.


\bibliography{references}

\end{document}
