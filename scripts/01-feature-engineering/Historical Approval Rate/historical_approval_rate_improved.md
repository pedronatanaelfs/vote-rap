# Historical Approval Rate (Proposition History) Feature Engineering

This script engineers **proposition-history features** for VOTE-RAP by looking at how the **same `propositionID`** performed in **previous voting sessions** and using that history to produce a simple, deterministic baseline prediction.

It is a Python script version of the original notebook (`related_objects.ipynb`) and generates:

1. `proposition_history_predictions_historical_probability_rule.csv`
2. `proposition_history_rules_comparison.png`
3. `historical_approval_rate_output.txt` (execution log)

---

## What the script does (as implemented)

### Input
Loads `data/vote_sessions_full.csv`, then:

- drops duplicate session ids: `drop_duplicates(subset=['id'])`
- parses `data` as datetime
- sorts deterministically by `['data', 'id']`

The script uses the following key columns:

- `id`: voting session id
- `data`: session date
- `propositionID`: proposition identifier
- `aprovacao`: session outcome (target; 0/1)

---

## Proposition history features

For **each session** `i`, the script finds **previous votes only**:

> `previous_votes = data[(propositionID == current_prop_id) & (data < current_date)]`

So “previous” is defined strictly by **earlier date** (`<`), not by file order and not by “earlier on the same day”.

If at least one `previous_vote` exists, the script computes:

- **`last_vote_result`**: the most recent previous `aprovacao`
- **`historical_approval_rate`**: mean of `aprovacao` across all previous votes
- **`vote_count`**: number of previous votes
- **`approval_trend`**: only if `vote_count >= 4`  
  Split previous votes into first half vs second half, then  
  `trend = mean(second_half) - mean(first_half)`
- **`rejection_streak`**: consecutive trailing 0s in the previous outcomes list
- **`approval_streak`**: consecutive trailing 1s in the previous outcomes list

If there is **no previous history**, the feature defaults are:

- `vote_count = 0`
- streaks = 0
- `last_vote_result`, `historical_approval_rate`, `approval_trend` remain `NaN`

---

## Prediction rules tested (exact behavior)

Rules are evaluated only on rows where:

- `vote_count > 0` **and**
- `aprovacao` is not null

Each rule outputs a **prediction score** (sometimes a probability, sometimes hard 0/1, sometimes 0.5 “unknown”).  
Accuracy is computed by thresholding: `(prediction > 0.5)`.

### Rules

1) **Momentum Rule (Same as Last Vote)**  
- if no last vote → 0.5  
- else → `last_vote_result`

2) **Contrarian Rule (Opposite of Last Vote)**  
- if no last vote → 0.5  
- else → `1 - last_vote_result`

3) **Historical Average Rule (> 0.5 threshold)**  
- if no history → 0.5  
- else → `1` if `historical_approval_rate > 0.5` else `0`

4) **Historical Probability Rule (BEST)**  
- if no history → 0.5  
- else → `historical_approval_rate` (used as a probability)

5) **Rejection Streak Rule (threshold = 2)**  
- if `rejection_streak >= 2` → predict `1`  
- else if `approval_streak >= 2` → predict `0`  
- else → `0.5`

6) **Rejection Streak Rule (threshold = 3)**  
Same as above, but with threshold 3.

7) **Trend Rule (Positive trend → Approval)**  
- if `approval_trend` is NaN → 0.5  
- else → `1` if `approval_trend > 0` else `0`

8) **Experience Rule (More votes → Higher approval probability)**  
- if `vote_count == 0` → 0.5  
- else → `0.5 + 0.3 * min(vote_count / 10, 1)`  
  (ranges from 0.5 to 0.8)

---

## Results (from the logged run)

From `historical_approval_rate_output.txt`:

Dataset:
- Total sessions: **9,260**
- Date range: **2003-02-19** to **2024-12-19**
- Unique `propositionID`: **2,118**
- Overall approval rate (`aprovacao.mean()`): **0.795**

History coverage:
- Sessions with some history (`vote_count > 0`): **4,867** (**52.6%**)

Rule evaluation sample size:
- Sessions scored by rules: **4,574** (these are rows with `vote_count > 0` and non-null `aprovacao`)

Best rule:
- **Historical Probability Rule**
  - AUROC: **0.6075**
  - Accuracy: **0.683**
  - Samples: **4,574**

The script also prints historical approval rate stats for rows with history:
- Mean: **0.871**
- Std: **0.240**
- Median: **1.000**
- Min/Max: **0.000 / 1.000**
- Qualitative shape reported by the script: **bimodal** (peaks at 0 and 1)

---

## Output files

### 1) `proposition_history_predictions_historical_probability_rule.csv`

This is the full 9,260-row dataset with history features and a `prediction` column generated by the best-performing rule.

Columns (in the exact order written by the script):
- `id`
- `data` (YYYY-MM-DD)
- `propositionID`
- `aprovacao`
- `vote_count`
- `last_vote_result`
- `historical_approval_rate`
- `approval_trend`
- `rejection_streak`
- `approval_streak`
- `prediction`
- `prediction_rule`

Notes:
- Rows with `vote_count == 0` have `prediction = 0.5` by construction.
- `approval_trend` is only defined when `vote_count >= 4`.

### 2) `proposition_history_rules_comparison.png`

A bar chart comparing AUROC across the 8 rules, highlighting the best one.

### 3) `historical_approval_rate_output.txt`

Execution log (stdout mirrored to file), including the printed metrics above.

---

## Interpretation (what this feature means)

For a session with `propositionID = P`, the key quantity is:

- `historical_approval_rate`: the fraction of prior sessions of proposition **P** (on earlier dates) that were approved.

Informally:
- values near **1.0** mean the proposition has historically been approved when it appeared in previous sessions;
- values near **0.0** mean it was historically rejected;
- values in-between indicate mixed history;
- `0.5` is the script’s neutral default when there is no usable history.

---

## Reproducibility guarantees (as implemented)

- `numpy.random.seed(42)` is set (even though the rules themselves are deterministic)
- sessions are sorted by `['data', 'id']`

This makes feature generation and metric results repeatable given the same input file.

---

*Last updated: 2025-11-28 (based on the execution log)*
